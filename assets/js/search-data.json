{
  
    
        "post0": {
            "title": "Data visualization gallery",
            "content": "Social justice &#9878;&#65039; . Climate, energy and environment &#127758; . . Note: The 2020 wildfire season isn&#8217;t fully captured here because this data is only current until late September 2020. . Economy and finance &#128176; . 1. Police and census data is sourced from Mapping Police Violence, The Washington Post and the United States Census Bureau.‚Ü© . 2. Climate data is sourced from the EPA, Cal Fire and the Global Carbon Project (Friedlingstein et al. 2020).‚Ü© . 3. Economic data is sourced from Opportunity Insights.‚Ü© .",
            "url": "https://stevhliu.github.io/ingolmo/altair/data%20vis/2020/12/31/data-viz-gallery.html",
            "relUrl": "/altair/data%20vis/2020/12/31/data-viz-gallery.html",
            "date": " ‚Ä¢ Dec 31, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "astroGPT",
            "content": "Introduction . In the last post, I shared some notes on understanding the Transformer, GPT-2 and different decoding strategies for generating text. Now for something a little more practical (and fun! üòä), let&#39;s fine-tune GPT-2 to generate horoscopes. Want to generate your daily horoscope?AstroGPT ü™ê ‚Äì by @stevhliuGPT-2 model fine-tuned on Western zodiac signsInput today&#39;s date on:üîÆ https://t.co/uCUVsmgmE7 . &mdash; Hugging Face (@huggingface) September 5, 2020 . Data . To start, we need some horoscopes to fine-tune the model with. The only publicly available dataset I could find was from a similar project that pulled horoscopes from The New York Post. However, it didn&#39;t contain as much text as I had hoped for, so I built my own scraper to collect data from Horoscope.com. The dataset contains a years worth of horoscopes for each of the twelve zodiac signs across several categories (daily, love, wellness, career). Once we have the data, we can create a training and validation set using a 80/20 split (all_text contains all the text). . num = int(0.8*len(all_text)) idxs = np.random.randint(0, len(all_text), len(all_text)) idxs_train = idxs[:num] idxs_val = idxs[num:] train = all_text.iloc[idxs_train] test = all_text.iloc[idxs_val] . Training the fastai way . The next step is to fine-tune GPT-2 on the horoscopes. fastai makes this part really simple, and offers amazing utilities like the learning rate finder, discriminative fine-tuning and 1cycle training. If you aren&#39;t familiar with these concepts, I highly recommend taking the fastai course, Practical Deep Learning for Coders, by Jeremy Howard and Sylvain Gugger üëè. . I&#39;ll just provide a brief summary of these ideas here, so I don&#39;t spoil anything in case you&#39;re interested in the course! . Learning rate finder . In the past, selecting an optimal learning rate has been challenging. If your learning rate is too high it will blow your training up, but if it is too low the model will train very, very slowly. So in 2015, Leslie Smith came up with the learning rate finder (see this paper for the details). The idea is simple: begin training the model with a small learning rate and then gradually increase it until the loss gets worse and worse (we record and plot the loss after each mini-batch to generate a chart). All we have to do then is pick a learning rate where the loss is decreasing. The general rule of thumb is to pick an order of magnitude less than the point where things begin to get worse. . Discriminative fine-tuning . Discriminative learning rates was introduced in ULMFiT by Jeremy Howard and Sebastian Ruder. It is based on the observation that not every layer of the neural net should be trained with the same learning rate. The earliest layers of a neural net detects basic patterns, while the later layers learn more complex things. In other words, different layers of the neural net represent different levels of semantic complexity. So if the early layers are already pretty good at recognizing these simple patterns, then we shouldn&#39;t change their weights too much (set a lower learning rate). But for the more complex patterns in the later layers, we use a higher learning rate so that they train and learn better. . 1cycle training . This is another amazing idea from Leslie Smith for training neural nets. Simply put, you begin training with a lower learning rate and then gradually increase that to some maximum value. This way, you avoid letting your training get out of hand and instead discover increasingly better parameters. Once you get to a sweet spot on the loss landscape, you lower the learning rate again to really hone in on the best parameters. Combined, this warming up and cooling down is known as 1cycle training and it allows you to train faster and more accurately. . fastai and &#129303; Hugging Face . To get started, download the pre-trained GPT-2 model from Hugging Face. . from transformers import GPT2LMHeadModel, GPT2TokenizerFast pretrained_weights = &#39;gpt2&#39; tokenizer = GPT2TokenizerFast.from_pretrained(pretrained_weights) model = GPT2LMHeadModel.from_pretrained(pretrained_weights) . Next, build a Transform that tokenizes the text. Luckily for us, Sylvain has already demonstrated how we can use fastai with Hugging Face and provided a tutorial that we can follow. . all_text = np.concatenate([df_train[&#39;text&#39;].values, df_valid[&#39;text&#39;].values]) class TransformersTokenizer(Transform): def __init__(self, tokenizer): self.tokenizer = tokenizer def encodes(self, x): toks = self.tokenizer.tokenize(x) return tensor(self.tokenizer.convert_tokens_to_ids(toks)) def decodes(self, x): return TitledStr(self.tokenizer.decode(x.cpu().numpy())) . Then you specify the training and validation sets. . splits = [list(range_of(df_train)), list(range(len(df_train), len(all_text)))] . Put together everything we need to handle our data: . all the text data ‚úÖ | the GPT-2 tokenizer ‚úÖ | the training and validation splits ‚úÖ | set the dataloader type to LMDataLoader because we&#39;re using a language model ‚úÖ | . tls = TfmdLists(all_text, TransformersTokenizer(tokenizer), splits=splits, dl_type=LMDataLoader) . Create a DataLoader and set the batch size and sequence length. You may have to adjust your batch size depending on your GPU memory. GPT-2 was trained on sequences of size 1024 so we will just keep it as it is. . bs,sl = 2,1024 dls = tls.dataloaders(bs=bs, seq_len=sl) . To get Hugging Face to work with fastai, we add a minor modification to the training loop with a callback. The Hugging Face model returns a tuple (see here) that contains the predictions and some other things. We only want the predictions so we drop the other stuff with a callback. . class DropOutput(Callback): def after_pred(self): self.learn.pred = self.pred[0] . Then create a Learner object which contains the data, model, loss function, the custom callback and a metric for evaluating the language model. We use mixed precision to train faster and save some memory. . learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), cbs=[DropOutput], metrics=[Perplexity()]).to_fp16() . Use the learning rate finder to discover a good learning rate. . learn.lr_find() . Once you&#39;ve picked a good learning rate, we do discriminative learning rates and 1cycle training. . learn.fit_one_cycle(2, 5e-3) learn.fit_one_cycle(10, lr_max=slice(1e-7, 1e-5)) learn.fit_one_cycle(20, lr_max=slice(1e-7, 1e-5)) . Here, you basically train as long as you can until the validation loss doesn&#39;t get any better. With my setup, I was able to get the validation loss down to 2.64 and the perplexity to 14.04. This took roughly 2.5 hours on one of Colab&#39;s GPU&#39;s. . And just like that you have a fine-tuned GPT-2 model! ü•≥ . Generating horoscopes &#128302; . After you&#39;re done fine-tuning your model, you can upload it to the Hugging Face Model Hub so that everyone can easily use it. Just follow the steps here. . Now for the fun part, load the model and generate your horoscope! . from transformers import AutoTokenizer, AutoModelWithLMHead tokenizer = AutoTokenizer.from_pretrained(&quot;stevhliu/astroGPT&quot;) model = AutoModelWithLMHead.from_pretrained(&quot;stevhliu/astroGPT&quot;) model.eval() model.to(&#39;cuda&#39;) # input the date as Mon DD, YYYY input_ids = tokenizer.encode(&#39;Sep 23, 2020&#39;, return_tensors=&#39;pt&#39;).to(&#39;cuda&#39;) sample_output = model.generate(input_ids, do_sample=True, max_length=75, top_k=20, top_p=0.97) print(tokenizer.decode(sample_output.tolist(), skip_special_tokens=True)) .",
            "url": "https://stevhliu.github.io/ingolmo/gpt2/huggingface/nlp/2020/09/23/astroGPT.html",
            "relUrl": "/gpt2/huggingface/nlp/2020/09/23/astroGPT.html",
            "date": " ‚Ä¢ Sep 23, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Transformers, GPT-2 and text generation",
            "content": "Introduction . Hello! I wanted to share my notes that helped me understand the Transformer, GPT-2 and how they can be used for text generation. This was really helpful because writing it down, and then explaining it in my own words helped me learn so much better. . The primary resources I used are: . The Illustrated Transformer | The Annotated Transformer | The Illustrated GPT-2 | ü§ó Hugging Face docs | . We will kick things off with the Transformer, since GPT-2 is just a variant of it. . Transformer . The Transformer&#39;s secret sauce comes from stacking self-attention layers in the encoder-decoder. Attention is exactly what it sounds like, it gives the model the ability to focus on a relevant subset of information. For example, in the sentence below, attention helps the model understand that one refers to ATMs. . There are two ATMs in Antarctica and only one of them works. . So in a nutshell, self-attention allows the model to look at all the other words (also known as attend to) in the sentence which lets the Transformer understand how they&#39;re all related to the word it is currently processing. . Self-attention . Secret sauce recipe: . Multiply each word&#39;s embedding by three weight matrices to get a query, key and value vector for each word. | Next calculate a score for each word so the model knows how much focus to place on each of them. Suppose we are encoding the word ATMs. | Take the dot product of the query and key for ATMs. Then multiply the query vector of ATMs by the key vector of all the other words in the sentence. Now we have a score for every word as it relates to ATMs. | Divide all these scores by a scaling factor that is equal to the square root of the dimension of the key vector. As the dimensionality of the query and key vectors increase, the size of the dot product also increases. To prevent this from blowing up and destabilizing the gradient, we need to rescale these scores. The authors call this scaled dot-product attention. | Pass these values through softmax so that all the scores sum to 1. At this point, we expect relevant words to have higher scores. | Multiply the softmax scores by their respective value vectors so we amplify the high scoring words and dampen the irrelevant ones. | Sum up all these values and you get a vector that contains the encoding for ATMs. | In practice, we do this calculation with matrices instead of vectors because it is much faster that way. . . Multi-head self-attention . Mapping the relationship between ATMs and one is just one representation of the sentence (this is learned by one attention head). What if we wanted to know where one is? In this case, it would be better for the Transformer to focus on Antarctica and one, instead of ATMs. So the Transformer is equipped with 8 attention heads to learn and capture all these different relationships from different positions, giving it a richer understanding of the sentence. This makes up the multi-head self-attention sub-layer in each encoder-decoder stack. . The way we calculate multi-head self-attention is similar to how we calculate self-attention. We do the same calculation 8 different times, each with their own separate weights to generate a bunch of encodings. Then we concatenate these together, multiply it by another set of weights and you end up with a matrix that contains the encoding of a word from multiple positions. . . We can see these learned representations with exBERT and visualize what each head is doing. So here, notice how head 1 and head 5 (highlighted in blue) are focusing on different parts of the sentence. . . Recent research however suggests that not all of these heads are necessary, and disabling one does not harm the performance of the Transformer because some of the information is redundant (The Dark Secrets of BERT). . Masked multi-head self-attention . But if the Transformer can attend to all the other words, how do we know it&#39;s actually learning and not just repeating what it has already seen? The trick is to implement an attention mask that prevents the decoder from seeing what the next word is by setting it to -inf. The mask is applied after we&#39;ve calculated the scores by multiplying the query and key vectors. So now when we take the softmax, the values in the mask go to 0 and the decoder can&#39;t attend to those words. The attention mask can be implemented like this (from The Annotated Transformer): . def subsequent_mask(size): attn_shape = (1,size,size) subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype(&#39;uint8&#39;) return torch.from_numpy(subsequent_mask) == 0 . . Model architecture . Now that we understand multi-head and masked multi-head self-attention, it&#39;s easier to understand how they fit into the overall architecture of the Transformer. The Transformer stacks 6 encoder-decoders which are pretty much identical except the decoder has an additional masked multi-head self-attention sub-layer. . . Encoder . The encoder has two sub-layers: multi-head self-attention and a fully connected feed-forward network. Between these sub-layers are a residual connection followed by a layer normalization step (check out the fast.ai course if you need a refresher). The Transformer also adds positional encoding values to the input embeddings because it doesn&#39;t know the position of words in the sequence. So positional encodings are a way to explicitly tell the Transformer where words are with respect to one other. . In brief, the encoder does a bunch of parallelized matrix multiplications (which is why the Transformer is so fast ‚ö°Ô∏è) that gets passed off to the encoder above it. By stacking these blocks together, you create a more powerful network with richer representations. . Decoder . The decoder is pretty much identical to the encoder except it has a masked multi-head self-attention sub-layer. The self-attention sub-layer takes the key and values from the top encoder and it gets the query matrix from the masked attention sub-layer below it. Then we just do the same self-attention calculation. The output of each time-step is used by the bottom decoder at the next time-step to generate the next word. Like the encoder, we add positional encodings to indicate a word&#39;s location. . Linear + Softmax . Lastly, the decoder outputs a vector of numbers that are passed into a linear layer which produces a vector of logits (this is the same size as the model vocabulary). Pass this through a softmax to normalize the values so that they all sum to 1, and then select the highest value to get the predicted word. . At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next (Vaswani et al.). . GPT-2 . GPT-2 is a Transformer-based model, but instead of the encoder-decoder architecture, it gets rid of the encoder entirely. It is basically a big stack of decoders with 1.5 billion parameters, a souped-up version of the original GPT which had 110 million parameters. In late May 2020, OpenAI introduced GPT-2&#39;s successor, GPT-3, which is an absolute unit with 175 billion parameters ü§Ø. The general strategy at OpenAI, when it comes to the GPT family, is to train bigger models on larger datasets. GPT-2 is a causal language model, meaning that it was trained to simply predict the next word in a sentence. The rationale is that if the model is able to predict the next word sufficiently well, this suggests that the model understands the text. For example, consider the sentence: . Armadillos can hold their breath for up to six minutes and are known to walk _____. . You&#39;d probably say underwater because why else would it hold it&#39;s breath? So in order for GPT-2 to successfully predict underwater, it would have to understand that the armadillo is somewhere it can&#39;t breathe regularly. In turn, this broad understanding of text carries over to being able to perform well on various NLP tasks (Q&amp;A, summarization, reading comprehension, etc.) without having to fine-tune the model to these tasks. This is known as zero-shot learning, meaning you can just use the model out of the box. . Model overview . During evaluation, GPT-2 is predicting one word at a time so it would be a waste to redo the self-attention calculations every step for tokens that have already been processed. Instead GPT-2 just keeps the key and value vectors for these tokens so that in the next step, it doesn&#39;t need to generate a new query, key and value vector. It simply reuses the ones it had saved from before. . Let&#39;s go through the sentence below to really solidify our understanding of how GPT-2 selects the next word. . Armadillos can hold their breath for up to six minutes and are known to walk _____. . Create the embedding and positional encodings for walk, and multiply that by a weight matrix to produce a concatenated vector of the query, key and values. | Split this into the query, key and value vectors and then reshape these vectors into a matrix. Next, split this matrix up for each attention head. | Then we do the usual attention calculation so that we score the current word against all the previous ones. This calculation is happening in each attention head. | Once we have all the scores from each attention head, we concatenate them into a single vector. Then we multiply this by another weight matrix that maps the outputs of the attention head into an output vector that we send to the feed-forward sub-layer. | The feed-forward network has two layers that the score vector passes through after which we do the usual softmax and get the probabilities over tokens. | . Byte-level byte-pair encoding (BPE) . Traditional tokenization splits a sentence into individual words, but you could end up with a huge vocabulary resulting in memory issues. If we tokenize each character individually, then we run into the problem of losing some of the semantics of the sentence. As a result, the model doesn&#39;t learn the representations as well. Instead, we split the difference and do subword tokenization where we try and cover all the words in our vocabulary by building up the smallest collection of subwords possible. This generally means that the more common words will be tokenized as a whole word, while rarer ones are broken down into smaller chunks. For example, permutation has been split into perm and utations here. . from transformers import GPT2Tokenizer tokenizer = GPT2Tokenizer.from_pretrained(&#39;gpt2&#39;) print(tokenizer.tokenize(&#39;There are more permutations of a standard deck of 52 cards than there are seconds since the Big Bang.&#39;)) [&#39;There&#39;, &#39;ƒ†are&#39;, &#39;ƒ†more&#39;, &#39;ƒ†perm&#39;, &#39;utations&#39;, &#39;ƒ†of&#39;, &#39;ƒ†a&#39;, &#39;ƒ†standard&#39;, &#39;ƒ†deck&#39;, &#39;ƒ†of&#39;, &#39;ƒ†52&#39;, &#39;ƒ†cards&#39;, &#39;ƒ†than&#39;, &#39;ƒ†there&#39;, &#39;ƒ†are&#39;, &#39;ƒ†seconds&#39;, &#39;ƒ†since&#39;, &#39;ƒ†the&#39;, &#39;ƒ†Big&#39;, &#39;ƒ†Bang&#39;, &#39;.&#39;] . This is how the BPE algorithm works: . Get the frequency of each word. | Create a list of the frequencies of each character (we call them tokens). | Merge the most common pair of tokens (also known as byte pairs, hence BPE). | Add this pair to your list of tokens and then recount the frequencies for each token. | Repeat these steps until you reach your vocabulary size. For GPT-2, this is 50000, so we stop training the tokenizer after 50000 merges. | Anything that the tokenizer hasn&#39;t seen before will be tokenized as &lt;unk&gt; since it is not in the base vocabulary. . But GPT-2 takes this a step further by using BPE on the byte sequences themselves ü§Ø. The characters you see on screen are represented by Unicode code points. For example, the character H looks like U+0048 (we can ignore the U). So 0048 is broken down into 4 bytes here. There are 256 bytes available, which becomes the base vocabulary and then these can be merged together to represent any other token in the vocabulary. Now you can tokenize any text without ever needing an &lt;unk&gt; token. . One more thing, the ƒ† you see from the GPT-2 tokenizer actually represents a whitespace because the encoder doesn&#39;t like them, so we need to represent it with some bytes. This is literally from the encoder.py file in OpenAI&#39;s GPT-2 ü§¢: . And avoids mapping to whitespace/control characters the bpe code barfs on . Decoding methods &#128269; . There are several different decoding methods for generating text, covered in this excellent post by Patrick from Hugging Face, that I will briefly summarize here. . Greedy search . This method just strings words together based on the next word with the highest probability. But because it only considers the next word, it may miss a word two-steps ahead with an even higher probability. . Beam search . Beam search addresses this by checking out a pre-defined number of steps ahead. So if you set num_beams=3, it will consider words three-steps ahead and return the sequence with the highest probability. . But an issue shared by both greedy and beam search is that after a while, it may begin repeating itself. We can try and fix this by introducing a penalty if the models sees the same sequence of words more than once, like San Francisco. In this case, we would set the probability to zero the next time the model sees these two words. The downside though is that you&#39;d only be able to mention San Francisco once in your generated text. . Sampling . The next two methods are sampling-based methods, which means randomly picking the next word based on its conditional probability distribution. . Cows have best friends and get lonely if they are separated. . Here, have is sampled from the conditioned probability distribution of Cows, and best is sampled from Cows, have, and so on. The downside of this approach is that often times you can end up generating nonsense. We can try and make this better by lowering the temperature. This squishes the probability distribution so that you are more likely to pick a high probability word. Notice how if you set the temperature to zero, it essentially becomes a greedy search because you are just picking the next word with the highest probability. . Top-K sampling . This is the sampling strategy GPT-2 uses. What happens here is that the top K most likely words are selected and the probability is redistributed among only these words. But the problem with this method is that it doesn&#39;t adapt well to the probability distribution of the next word. . In the case of a flat distribution, by limiting to only K words, we can end up eliminating plausible word candidates which can hinder the models creativity. | In the case of a sharp distribution, we run the risk of including some words that don&#39;t make sense in the sampling pool. | . Top-p sampling . The issue here is that by fixing K to a certain number of words, we can&#39;t adapt when the next word probability distribution changes. So instead of setting K, we can try and set p instead, where p is the probability. What this means is that we find the set of words whose cumulative probability exceeds p. Then we redistribute the probability amongst this set of words. In doing so, the number of words in the next set (as opposed to a fixed K) can vary according to the probability distribution. . What this translates to is that when the next word is less predictable, you have a wider range of words to sample from, giving you more options to be creative. When the next word is more predictable, you&#39;ll just get less words because it only needs to pick a few words to exceed p. .",
            "url": "https://stevhliu.github.io/ingolmo/transformer/gpt2/huggingface/nlp/2020/09/08/transformer-gpt2.html",
            "relUrl": "/transformer/gpt2/huggingface/nlp/2020/09/08/transformer-gpt2.html",
            "date": " ‚Ä¢ Sep 8, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Covid-19 dashboard",
            "content": ". Important: The COVID Tracking Project will be ending on March 7, so most of these charts (aside from the geographical and vaccine ones) will no longer be updating on a daily basis. I started this project to learn Altair, provide high-quality and transparent public health data at a time when the federal government seemed incapable of doing so, and show the pandemic&#8217;s impact in my home state of California. This has been a powerful experience and sometimes it is easy to view these numbers as just numbers. But it serves as an important reminder that these incredible numbers represent people who have lost their lives. I will keep this dashboard around so we can look back on this historic moment, and I am very grateful to the COVID Tracking Project team for their hard work and efforts in collecting and publishing their data! ‚ù§Ô∏è . 3501394 cases &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;54124 deaths . Confirmed in California as of March 17 2021 . Explain Like I&#39;m Five: How does testing work? . You&#39;ve probably heard a lot about testing lately so I thought it&#39;d be useful to understand the science behind how it actually works. There are several types of tests out there, all of which detect different things. For example, an antibody test tells you whether you&#39;ve been previously infected based on the presence of certain proteins that are generated in response to the virus. Another type of test is the PCR (polymerase chain reaction), and this is what you&#39;ll probably get if you&#39;re being tested. . So this is basically how PCR works: . You start by collecting a sample, like a nose swab. If the virus is present, some of it will probably be trapped there. | The coronavirus is a RNA virus, so you need to separate the RNA from all the other biological stuff in the sample. There are a bunch of different ways you can do this and when I was in college, we used a phenol-chloroform extraction. What happens is that things other than RNA gets destroyed, and in the presence of certain chemicals, stuff separates out into something like a 3 layer bean dip. The RNA will be separated out into a certain layer, so now you can just extract it from there. | Since you are starting out with RNA, you actually have to do reverse transcription PCR (RT-PCR), to first make complementary DNA (cDNA) from it. Once you have your cDNA template, then you can do regular PCR. You give it some primers (short strands of DNA to give the reaction a starting point), some basic building materials and an enzyme that puts all this stuff together for you. | Next you have to get the temperature right. You can think of the temperature as a sequence of conditions your workers require in order to function. It has to be high enough to separate the DNA strands, cool enough so the primers can bind, and then high again for your enzyme to start building out the new strand. | The thermal cycler takes care of all this and repeats this step multiple times, so you basically end up copying/pasting until you have way more DNA than you started out with. | The key part here is that you choose the right primers (essentially fragments of the virus) that will bind to the cDNA of your sample. In other words, if there is no Covid-19 in the sample, then you won&#39;t get anything. | . 1. Data is sourced from The COVID Tracking Project, The New York Times, OWID and the United States Census Bureau.‚Ü© .",
            "url": "https://stevhliu.github.io/ingolmo/altair/data%20vis/covid19/2020/04/05/covid19-in-ca.html",
            "relUrl": "/altair/data%20vis/covid19/2020/04/05/covid19-in-ca.html",
            "date": " ‚Ä¢ Apr 5, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". I am a former molecular/cell biologist working in fintech to help companies build and launch financial products. I like to make charts and graphs from data-rich sources that tell meaningful stories about complex topics like climate change, the economy and social justice to name just a few. I also enjoy studying NLP and contributing new language models or datasets to Hugging Face. . When I‚Äôm not doing nerdy data things, I like to travel to see and learn more about the world, it‚Äôs people and culture. I am especially drawn to outdoor places like National Parks and my goal is to visit all of them someday. I like to cook and bake because I enjoy making things for other people, tasting cool new coffees and practicing my homebrewing, cheering on the Golden State Warriors, playing board games, being a plant parent, petting dogs and searching for the biggest &amp; baddest burritos. . Feel free to say hello on Twitter or LinkedIn! .",
          "url": "https://stevhliu.github.io/ingolmo/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://stevhliu.github.io/ingolmo/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}